{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    indices = []\n",
    "    syllables = []\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as fp:\n",
    "        raw_data = fp.readlines()\n",
    "        for datum in raw_data:\n",
    "            idx, syllable, _ = datum.split('\\t')\n",
    "            indices.append(idx)\n",
    "            syllables.append(syllable)\n",
    "\n",
    "    return indices, syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('ner_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "\n",
    "def convert_sentence(sentence):\n",
    "    return sentence.replace(' ', '').replace('<SP>', ' ')\n",
    "\n",
    "\n",
    "def tag_pos(sentence):\n",
    "    syllable = sentence.split(' ')\n",
    "    converted = convert_sentence(sentence)\n",
    "    morpher = Mecab()\n",
    "    morph_pos_list = morpher.pos(converted)\n",
    "    morphs_list = []\n",
    "\n",
    "    pos_list = []\n",
    "    index = 0\n",
    "\n",
    "    for morph_pos in morph_pos_list:\n",
    "        morph, pos = morph_pos\n",
    "        morphs_list.append(morph)\n",
    "        pos_list.append('B_{}'.format(pos))\n",
    "\n",
    "        for i in range(1, len(morph)):\n",
    "            pos_list.append('I_{}'.format(pos))\n",
    "\n",
    "        index += len(morph)\n",
    "        if index < len(syllable):\n",
    "            if syllable[index] == '<SP>':\n",
    "                morphs_list.append('<SP>')\n",
    "                pos_list.append('<SP>')\n",
    "                index += 1\n",
    "\n",
    "    return morphs_list, pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1360\n1360\n1360\n1360\n"
     ]
    }
   ],
   "source": [
    "morphs_pos_list = [tag_pos(syllable) for syllable in syllables]\n",
    "morphs_list = [m[0] for m in morphs_pos_list]\n",
    "pos_list = [m[1] for m in morphs_pos_list]\n",
    "\n",
    "print(len(indices))\n",
    "print(len(syllables))\n",
    "print(len(morphs_list))\n",
    "print(len(pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['test_morphs.pickle']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pos_list, 'test_pos.pickle')\n",
    "joblib.dump(morphs_list, 'test_morphs.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchcrf import CRF\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "class RNN_CRF(nn.Module):\n",
    "    def __init__(self, config, p_dist, idx2word):\n",
    "        super(RNN_CRF, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n",
    "        self.word_embedding_size = config[\"embedding_size\"] \n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.number_of_tags = config[\"number_of_tags\"]\n",
    "        self.number_of_heads = config[\"number_of_heads\"]\n",
    "        self.NE_idx = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
    "                                        embedding_dim=self.word_embedding_size,\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "        self.NE_embedding = torch.nn.Embedding(num_embeddings=config[\"number_of_tags\"],\n",
    "                                        embedding_dim=config[\"number_of_tags\"],\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        \n",
    "        self.p_dist = p_dist\n",
    "        \n",
    "        self.idx2word = idx2word\n",
    "\n",
    "        with open(\"total_glove_emb.pickle\", \"rb\") as fr:\n",
    "            self.emb = pickle.load(fr)\n",
    "        \n",
    "        with open(\"pos_dict_ej.pickle\", \"rb\") as fr:\n",
    "            self.pos_onehot = pickle.load(fr)\n",
    "        \n",
    "        self.first_bi_gru = nn.GRU(input_size = 128 + 14 + 46,\n",
    "                             hidden_size= self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=False,\n",
    "                             bidirectional=True)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(self.hidden_size*2, self.number_of_heads,\n",
    "                                                    kdim=self.number_of_tags,\n",
    "                                                    vdim=self.number_of_tags)\n",
    "\n",
    "        self.second_bi_gru = nn.GRU(input_size = self.hidden_size*4,\n",
    "                             hidden_size= self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=False,\n",
    "                             bidirectional=True)\n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n",
    "\n",
    "        # # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n",
    "        # # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n",
    "        self.hidden2num_tag = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_tags)\n",
    "  \n",
    "        \n",
    "    def scaled_dot_product(self, query, key):\n",
    "        return torch.matmul(query, key) / math.sqrt(self.number_of_tags)\n",
    "    \n",
    "    def forward(self, inputs, labels=None):\n",
    "        NE_tensor = []\n",
    "        for i in range(len(inputs)):\n",
    "            NE_tensor.append(self.NE_idx)\n",
    "        NE_tensor = torch.tensor(NE_tensor, dtype=torch.long).cuda()\n",
    "        \n",
    "        p_dist_feature = []\n",
    "        for batch in inputs:\n",
    "            seq = []\n",
    "            for char in batch:\n",
    "                seq.append(self.p_dist[char])\n",
    "            p_dist_feature.append(seq)\n",
    "        p_dist_feature = torch.tensor(p_dist_feature, dtype=torch.float).cuda()\n",
    "        \n",
    "        emb_feature = []\n",
    "        for batch in inputs:\n",
    "            seq = []\n",
    "            for char in batch:\n",
    "                if self.idx2word[char.item()] in self.emb :\n",
    "                    seq.append( self.emb[self.idx2word[char.item()]] )\n",
    "                else:\n",
    "                    x = []\n",
    "                    for i in range(128):\n",
    "                        x.append(0)\n",
    "                    seq.append(x)\n",
    "            emb_feature.append(seq)\n",
    "        emb_feature = torch.tensor(emb_feature, dtype=torch.float).cuda()\n",
    "\n",
    "        pos_feature = []\n",
    "        for batch in inputs:\n",
    "            seq = []\n",
    "            for char in batch:\n",
    "                if self.idx2word[char.item()] in self.pos_onehot:\n",
    "                    seq.append(self.pos_onehot[self.idx2word[char.item()]])\n",
    "                else:\n",
    "                    x = []\n",
    "                    for i in range(46):\n",
    "                        x.append(0)\n",
    "                    seq.append(x)\n",
    "            pos_feature.append(seq)\n",
    "        pos_feature = torch.tensor(pos_feature, dtype=torch.float).cuda()\n",
    "\n",
    "        p_dist_feature = torch.cat((p_dist_feature, emb_feature), dim=2)\n",
    "        p_dist_feature = torch.cat((p_dist_feature, pos_feature), dim=2)\n",
    "        eumjeol_inputs = p_dist_feature.permute(1,0,2)\n",
    "\n",
    "        # eumjeol_inputs = self.word_embedding(inputs).permute(1, 0, 2)\n",
    "        NE_embedded = self.NE_embedding(NE_tensor).permute(1, 0, 2)\n",
    "\n",
    "        # eumjeol_inputs = torch.cat((eumjeol_inputs, p_dist_feature.permute(1, 0, 2)), dim=2)\n",
    "        # eumjeol_inputs = torch.cat((eumjeol_inputs, pos_feature.permute(1, 0, 2)), dim=2)\n",
    "        \n",
    "        encoder_outputs, hidden_states = self.first_bi_gru(eumjeol_inputs)\n",
    "        attn_output, attn_output_weights = self.multihead_attn(encoder_outputs, NE_embedded, NE_embedded)\n",
    "        decoder_input = torch.cat((encoder_outputs, attn_output), 2)\n",
    "        decoder_outputs = self.dropout(decoder_input)\n",
    "        decoder_outputs, decoder_hidden_states = self.second_bi_gru(decoder_input)\n",
    "        decoder_outputs = self.dropout(decoder_outputs)\n",
    "        decoder_outputs = self.hidden2num_tag(decoder_outputs)\n",
    "        logits = self.scaled_dot_product(decoder_outputs.permute(1, 0, 2), NE_embedded.permute(1, 2, 0))\n",
    "        \n",
    "        # if(labels is not None):\n",
    "        #     loss = 0\n",
    "        #     for i in range(len(score_mat)):\n",
    "        #         loss += F.cross_entropy(score_mat[i], labels[i])\n",
    "        #     return loss\n",
    "        # else:\n",
    "        #     return torch.argmax(F.softmax(score_mat,dim=2), dim=2).tolist()\n",
    "\n",
    "        if(labels is not None):\n",
    "            log_likelihood = self.crf(emissions=logits,\n",
    "                                      tags=labels,\n",
    "                                      reduction=\"mean\")\n",
    "            loss = log_likelihood * -1.0\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            output = self.crf.decode(emissions=logits)\n",
    "            return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-73a7b2ce94ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model1212'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/School/4-2/ai/homeworks/hackathon2/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/School/4-2/ai/homeworks/hackathon2/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('model1212')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    with open('Team6.pred', 'w', encoding='utf-8'):\n",
    "        for index, syllables in data:\n",
    "            syllables = source.split()\n",
    "            # word2idx 로 변환\n",
    "            source = []\n",
    "            output = model(source)\n",
    "\n",
    "            predictions = []\n",
    "            size = len(source)\n",
    "            for i in range(size):\n",
    "                predictions.append(idx_2_tag[i])\n",
    "\n",
    "            fp.write('{}\\t{}\\n'.format(index, ' '.join(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    indices = []\n",
    "    syllables = []\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as fp:\n",
    "        raw_data = fp.readlines()\n",
    "        for datum in raw_data:\n",
    "            idx, syllable, _ = datum.split('\\t')\n",
    "            indices.append(idx)\n",
    "            syllables.append(syllable)\n",
    "\n",
    "    return indices, syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "indices, syllables = load_data('ner_test.txt')\n",
    "answers = joblib.load('final_answer.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fe05345d2062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'6팀.pred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\t{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "with open('6팀.pred', 'w', encoding='utf-8') as fp:\n",
    "    for idx, answer in zip(indices, answers):\n",
    "        fp.write('{}\\t{}\\n'.format(idx, ' '.join([a for a in answer if a != '<PAD>'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('6팀.pred', 'w', encoding='utf-8') as fp:\n",
    "    for row in answer:\n",
    "        if row[-1] != '\\n':\n",
    "            row += '\\n'\n",
    "        fp.write(row)"
   ]
  }
 ]
}