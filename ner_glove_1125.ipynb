{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ner_glove_1125.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzDGgDdBmEMC"
      },
      "source": [
        "# 해커톤 2차 baseline + Glove\n",
        "- updated 1125\n",
        "- ref https://keep-steady.tistory.com/20\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7xrHyrKnpFW"
      },
      "source": [
        "## ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sORFEWv2mJY3",
        "outputId": "c65356a7-0ed4-42a7-a258-3cda22711655"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekeyg8SKmQys",
        "outputId": "47c834cb-fe85-46f8-d6f5-ae6410796c34"
      },
      "source": [
        "cd /content/drive/MyDrive/AI_Hackathon_konkuk2/baseline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/AI_Hackathon_konkuk2/baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uh5x9PPmYUU"
      },
      "source": [
        "# ready 한꺼번에 import \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "import os\n",
        "import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "from IPython.display import display\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_columns = 999\n",
        "\n",
        "import re\n",
        "\n",
        "# visualization\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# display\n",
        "from IPython.display import Image"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr_Htj0pUK4x"
      },
      "source": [
        "## Load data & Tokenizing\n",
        "- elmo -> tf 1.0 필요\n",
        "- https://github.com/lovit/soynlp (비지도학습 지향)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATS4QGPyUPaI",
        "outputId": "9eddbf9f-8d6a-42f8-bcc9-dad812cf1b34"
      },
      "source": [
        "!pip install glove_python"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: glove_python in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubG686Jxc0Qy"
      },
      "source": [
        "# !pip install soynlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3FDIiEhbhA8"
      },
      "source": [
        "### train data 로드하고 전처리 없이 형태소 분석한 후에 glove 로 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8uMDI3abp0c",
        "outputId": "42a6c88c-e939-4877-d6ed-92a06d80ab15"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.9MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, tweepy, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmsl3S8vb8zf"
      },
      "source": [
        "from konlpy.tag import * \n",
        "from gensim.models import Word2Vec, fasttext"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-CjV83Oj3wJ"
      },
      "source": [
        "# load data\n",
        "def load_data(path):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as fp:\n",
        "        lines = fp.readlines()\n",
        "        for line in lines:\n",
        "            _, sentence, label = line.strip().split('\\t')\n",
        "            cleaned_sentence = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣]+', ' ', sentence).strip()\n",
        "            if cleaned_sentence:\n",
        "                sentences.append(re.sub(' ','',str(cleaned_sentence)))\n",
        "               \n",
        "        return sentences\n",
        "\n",
        "train = load_data(\"/content/drive/MyDrive/AI_Hackathon_konkuk2/baseline/ner_train.txt\")\n",
        "\n",
        "# 1차 해커톤 문근님 함수 이용\n",
        "okt=Okt()  \n",
        "# sentence는 한 문장임.\n",
        "def tknz(sentence):\n",
        "    s = okt.pos(sentence)\n",
        "    x = []\n",
        "    for w in s:\n",
        "\t\t\t\t# w[0] : 단어 / w[1] = 품사\n",
        "\t\t\t\t# modifer 오타 실화? ~..~\n",
        "        if w[1] == 'Josa' or w[1] == 'Punctuation' or w[1] == 'Number' or w[1] == 'Modifer' or w[1] == 'Eomi':\n",
        "            continue\n",
        "        else: x.append(w[0])\n",
        "\t # 거를 거 거른 단어들의 리스트를 반환한다.\n",
        "    return x\n",
        "\n",
        "tokens = []\n",
        "for sentence in train:\n",
        "    x = tknz(str(sentence))\n",
        "    tokens.append(x)\n",
        "\n",
        "with open('train_token_1124.pickle', 'wb') as f:\n",
        "    pickle.dump(tokens, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPulScW5IJq0"
      },
      "source": [
        "# load token\n",
        "train_token = joblib.load(os.path.join('train_token_1124.pickle'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ4dG2xiUPNE"
      },
      "source": [
        "from glove import Corpus, Glove"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Rn2YY4b9pg"
      },
      "source": [
        "# corpus\n",
        "corpus = Corpus() \n",
        "corpus.fit(tokens, window=10)\n",
        "\n",
        "# model\n",
        "glove = Glove(no_components=128, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=10000, no_threads=4, verbose=False)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjAO487_mTbV"
      },
      "source": [
        "## Save & Load Glove\n",
        "sentence representation 보기위한 임베딩  \n",
        "오버피팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2kfxPab9t9"
      },
      "source": [
        "# # save\n",
        "# glove.save('glove_train_1124.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZE2SoYqZxN"
      },
      "source": [
        "https://www.kaggle.com/francoisdubois/build-a-word-embedding-with-glove-matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWuy1NLGBfj5"
      },
      "source": [
        "# load glove\n",
        "glove_model = Glove.load('glove_train_1124.model')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFQgyuj0b9r_",
        "outputId": "dfeeb5c1-2789-4762-d506-947430f9f82b"
      },
      "source": [
        "# word dict\n",
        "word_dict = {}\n",
        "for word in  glove_model.dictionary.keys():\n",
        "    word_dict[word] = glove_model.word_vectors[glove_model.dictionary[word]]\n",
        "\n",
        "print('Lengh of word dict... : ', len(word_dict))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengh of word dict... :  29084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL5VehkFLCki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5bd4dc-df91-456b-c40b-f48fc9478dc1"
      },
      "source": [
        "# embedding_vectors\n",
        "emb_mat = np.zeros((len(train_token),128))\n",
        "\n",
        "for i,morphs in enumerate(train_token):\n",
        "    vector = np.array([word_dict[morph] for morph in morphs])\n",
        "    #print(vector.shape)\n",
        "    final_vector = np.mean(vector,axis=0)\n",
        "    #final_vector = vector.T.mean(axis=1)\n",
        "    #emb_sentences.append(final_vector)\n",
        "    emb_mat[i] = final_vector\n",
        "\n",
        "print('Eebedding vector 120 dim.... : ', emb_mat.shape)\n",
        "\n",
        "# 저장을 원한다면 pickle 파일로"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eebedding vector 120 dim.... :  (7314, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ2rRLeJXcbf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmm9hD1_Xcdz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeKcybrwnsbL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK7R3r404hWn"
      },
      "source": [
        "# ready\n",
        "!pip install pytorch-crf\n",
        "!pip install seqeval==1.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lKp_q_x0eyb"
      },
      "source": [
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "from torch.utils.data import (DataLoader, TensorDataset)\n",
        "import torch.optim as optim\n",
        "\n",
        "# eval\n",
        "from seqeval.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdBFg0kb4Urd"
      },
      "source": [
        "class RNN_CRF(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(RNN_CRF, self).__init__()\n",
        "\n",
        "        # 전체 음절 개수\n",
        "        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n",
        "\n",
        "        # 음절 임베딩 사이즈\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "\n",
        "        # GRU 히든 사이즈\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "\n",
        "        # 분류할 태그의 개수\n",
        "        self.number_of_tags = config[\"number_of_tags\"]\n",
        "\n",
        "        # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 객체\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
        "                                      embedding_dim=self.embedding_size,\n",
        "                                      padding_idx=0)\n",
        "\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        # Bi-GRU layer\n",
        "        self.bi_gru = nn.GRU(input_size = self.embedding_size,\n",
        "                             hidden_size= self.hidden_size,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "\n",
        "        # CRF layer\n",
        "        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n",
        "\n",
        "        # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n",
        "        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n",
        "        self.hidden2num_tag = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_tags)\n",
        "\n",
        "    def forward(self, inputs, labels=None):\n",
        "        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "        eumjeol_inputs = self.embedding(inputs)\n",
        "\n",
        "        encoder_outputs, hidden_states = self.bi_gru(eumjeol_inputs)\n",
        "\n",
        "        # (batch_size, curr_max_length, hidden_size*2)\n",
        "        d_hidden_outputs = self.dropout(encoder_outputs)\n",
        "\n",
        "        # (batch_size, curr_max_length, hidden_size*2) -> (batch_size, curr_max_length, number_of_tags)\n",
        "        logits = self.hidden2num_tag(d_hidden_outputs)\n",
        "\n",
        "        if(labels is not None):\n",
        "            log_likelihood = self.crf(emissions=logits,\n",
        "                                      tags=labels,\n",
        "                                      reduction=\"mean\")\n",
        "\n",
        "            loss = log_likelihood * -1.0\n",
        "\n",
        "            return loss\n",
        "        else:\n",
        "            output = self.crf.decode(emissions=logits)\n",
        "\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmpN4ttRnygP"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlVPOWVk4nqP"
      },
      "source": [
        "# 파라미터로 입력받은 파일에 저장된 단어 리스트를 딕셔너리 형태로 저장\n",
        "def load_vocab(f_name):\n",
        "    vocab_file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "    print(\"{} vocab file loading...\".format(f_name))\n",
        "\n",
        "    # default 요소가 저장된 딕셔너리 생성\n",
        "    symbol2idx, idx2symbol = {\"<PAD>\":0, \"<UNK>\":1}, {0:\"<PAD>\", 1:\"<UNK>\"}\n",
        "\n",
        "    # 시작 인덱스 번호 저장\n",
        "    index = len(symbol2idx)\n",
        "    for line in tqdm(vocab_file.readlines()):\n",
        "        symbol = line.strip()\n",
        "        symbol2idx[symbol] = index\n",
        "        idx2symbol[index]= symbol\n",
        "        index+=1\n",
        "\n",
        "    print(f'total len of {f_name}... : ',len(symbol2idx))   # Add length\n",
        "    return symbol2idx, idx2symbol\n",
        "\n",
        "# 입력 데이터를 고정 길이의 벡터로 표현하기 위한 함수\n",
        "def convert_data2feature(data, symbol2idx, max_length=None):\n",
        "    # 고정 길이의 0 벡터 생성\n",
        "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
        "    # 입력 문장을 공백 기준으로 split\n",
        "    words = data.split()\n",
        "\n",
        "    for idx, word in enumerate(words[:max_length]):\n",
        "        if word in symbol2idx.keys():\n",
        "            feature[idx] = symbol2idx[word]\n",
        "        else:\n",
        "            feature[idx] = symbol2idx[\"<UNK>\"]\n",
        "    return feature\n",
        "\n",
        "# 파라미터로 입력받은 파일로부터 tensor객체 생성\n",
        "def load_data(config, f_name, word2idx, tag2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    # return할 문장/라벨 리스트 생성\n",
        "    indexing_inputs, indexing_tags = [], []\n",
        "\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
        "    # 문장 \\t 태그\n",
        "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
        "        indexing_tag = convert_data2feature(tags, tag2idx, config[\"max_length\"])\n",
        "\n",
        "        indexing_inputs.append(input_sentence)\n",
        "        indexing_tags.append(indexing_tag)\n",
        "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
        "    indexing_tags = torch.tensor(indexing_tags, dtype=torch.long)\n",
        "\n",
        "    # Add check load data\n",
        "    print('\\ncheck indexing_inputs... : ', indexing_inputs.shape, indexing_inputs[0])   \n",
        "    print('check indexing_tags... : ',indexing_tags.shape, indexing_tags[0])   \n",
        "\n",
        "    return indexing_inputs, indexing_tags\n",
        "\n",
        "# tensor 객체를 리스트 형으로 바꾸기 위한 함수\n",
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veTuM-Iyn3go"
      },
      "source": [
        "# Setting Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfKUI3mH4wqh"
      },
      "source": [
        "def train(config):\n",
        "    # 모델 객체 생성\n",
        "    model = RNN_CRF(config).cuda()\n",
        "    # 단어 딕셔너리 생성\n",
        "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
        "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
        "\n",
        "    # 데이터 Load\n",
        "    train_input_features, train_tags = load_data(config, config[\"train_file\"], word2idx, tag2idx)\n",
        "    test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
        "\n",
        "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
        "    train_features = TensorDataset(train_input_features, train_tags)\n",
        "    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    test_features = TensorDataset(test_input_features, test_tags)\n",
        "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # 모델을 학습하기위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    accuracy_list = []\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # .cuda()를 이용하여 메모리에 업로드\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "            input_features, labels = batch\n",
        "\n",
        "            # loss 계산\n",
        "            loss = model.forward(input_features, labels)\n",
        "\n",
        "            # 변화도 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "            loss.backward()\n",
        "\n",
        "            # 모델 내부 각 매개변수 가중치 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 50 == 0:\n",
        "                # Add epoch\n",
        "                print(f'epoch_{epoch + 1}_train')\n",
        "                print(\"{} step processed.. current loss : {}\".format(step + 1, loss.data.item()))\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Average Loss : {}\".format(np.mean(losses)))\n",
        "\n",
        "        # 모델 저장\n",
        "        torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
        "\n",
        "        do_test(model, test_dataloader, idx2tag)\n",
        "\n",
        "\n",
        "\n",
        "def test(config):\n",
        "    # 모델 객체 생성\n",
        "    model = RNN_CRF(config).cuda()\n",
        "    # 단어 딕셔너리 생성\n",
        "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
        "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
        "\n",
        "\n",
        "    # 저장된 가중치 Load\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "\n",
        "    # 데이터 Load\n",
        "    test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
        "\n",
        "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
        "    test_features = TensorDataset(test_input_features, test_tags)\n",
        "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
        "    # 평가 함수 호출\n",
        "    do_test(model, test_dataloader, idx2tag)\n",
        "\n",
        "def do_test(model, test_dataloader, idx2tag):\n",
        "    model.eval()\n",
        "    predicts, answers = [], []\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        # .cuda() 함수를 이용하요 메모리에 업로드\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # 데이터를 각 변수에 저장\n",
        "        input_features, labels = batch\n",
        "\n",
        "        # 예측 라벨 출력\n",
        "        output = model(input_features)\n",
        "\n",
        "        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n",
        "        for idx, answer in enumerate(tensor2list(labels)):\n",
        "            answers.extend([idx2tag[e].replace(\"_\", \"-\") for e in answer if idx2tag[e] != \"<SP>\" and idx2tag[e] != \"<PAD>\"])\n",
        "            predicts.extend([idx2tag[e].replace(\"_\", \"-\") for i, e in enumerate(output[idx]) if idx2tag[answer[i]] != \"<SP>\" and idx2tag[answer[i]] != \"<PAD>\"] )\n",
        "    \n",
        "    # 성능 평가\n",
        "    print(classification_report(answers, predicts))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-yeUT7Bn8vy"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXHsFV-z4zZc",
        "outputId": "cc8450cf-99a0-4a4a-e357-85efb300a883"
      },
      "source": [
        "##########################################################\n",
        "#                                                        #\n",
        "#        평가 기준이 되는 지표는 Macro F1 Score                #\n",
        "#           제출 포맷은 id \\t predict_tag                   #\n",
        "#            25 \\t B_PS I_PS <SP> O O O ...              #\n",
        "#                                                        #\n",
        "##########################################################\n",
        "\n",
        "\n",
        "import os\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"train_file\":\"ner_train.txt\",\n",
        "              \"dev_file\": \"ner_dev.txt\",\n",
        "              \"word_vocab_file\":\"vocab.txt\",\n",
        "              \"tag_vocab_file\":\"tag_vocab.txt\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(5),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"word_vocab_size\":2160,\n",
        "              \"number_of_tags\": 14,\n",
        "              \"hidden_size\": 100,\n",
        "              \"dropout\":0.2,\n",
        "              \"embedding_size\":100,\n",
        "              \"max_length\": 120,\n",
        "              \"batch_size\":64,\n",
        "              \"epoch\":5,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2158/2158 [00:00<00:00, 531866.73it/s]\n",
            "100%|██████████| 12/12 [00:00<00:00, 15491.43it/s]\n",
            " 27%|██▋       | 1986/7319 [00:00<00:00, 19849.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab.txt vocab file loading...\n",
            "total len of vocab.txt... :  2160\n",
            "tag_vocab.txt vocab file loading...\n",
            "total len of tag_vocab.txt... :  14\n",
            "ner_train.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7319/7319 [00:00<00:00, 19515.01it/s]\n",
            "100%|██████████| 995/995 [00:00<00:00, 22133.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "check indexing_inputs... :  torch.Size([7319, 120]) tensor([1432, 1349, 1847, 1734, 1511,   29, 1580, 1621, 1717,  866,   29,  856,\n",
            "        1709, 2048,  125, 1013, 1020,   29,   14,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
            "check indexing_tags... :  torch.Size([7319, 120]) tensor([ 7, 12, 13, 13, 13,  2, 13, 13, 13, 13,  2, 13, 13, 13, 13, 13, 13,  2,\n",
            "        13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
            "ner_dev.txt file loading...\n",
            "\n",
            "check indexing_inputs... :  torch.Size([995, 120]) tensor([  22, 1691,   29, 1674, 1962, 1597,  820, 1636,   29, 1724, 1674, 1597,\n",
            "         820, 1604,   29, 1102, 1213, 1270,   29,   95, 1688, 1230, 1974,   13,\n",
            "          51,   43,   96,   29,  774, 1990, 1683,   29,  951,  798,  122,   29,\n",
            "        1747, 1674, 1467,  772,   29, 1631, 2057,   29, 1562, 1604,   29, 1091,\n",
            "        1709, 2048,  123,   29,  801, 1688,   29, 2092, 1515, 1511, 1066, 1701,\n",
            "          29, 1688, 1230, 1974, 1636,   29,  818, 1714,  833,  820, 1604,   29,\n",
            "        1697, 1005,   29, 1193, 1050, 1230, 1974,   29, 2087, 2039, 1162, 1502,\n",
            "          29, 1091,   29, 1020, 1215,   29, 1034, 2081,   29, 1230, 1974, 1056,\n",
            "          29, 1244, 1709,   29,  951,   29, 1747, 1674, 1467,   29, 1449, 1227,\n",
            "        1683,   29, 1667, 2048, 1560,   29, 1724, 1674, 1422, 1636,   29, 1289])\n",
            "check indexing_tags... :  torch.Size([995, 120]) tensor([ 3,  8,  2, 13, 13, 13, 13, 13,  2, 13, 13, 13, 13, 13,  2, 13, 13, 13,\n",
            "         2, 13, 13, 13, 13, 13, 13, 13, 13,  2, 13, 13, 13,  2, 13, 13, 13,  2,\n",
            "        13, 13, 13, 13,  2,  3,  8,  2, 13, 13,  2, 13, 13, 13, 13,  2, 13, 13,\n",
            "         2, 13, 13, 13, 13, 13,  2,  5, 10, 10, 13,  2, 13, 13, 13, 13, 13,  2,\n",
            "        13, 13,  2,  5, 10, 10, 10,  2,  5, 10, 10, 10,  2, 13,  2, 13, 13,  2,\n",
            "        13, 13,  2,  5, 10, 13,  2, 13, 13,  2, 13,  2, 13, 13, 13,  2, 13, 13,\n",
            "        13,  2, 13, 13, 13,  2, 13, 13, 13, 13,  2, 13])\n",
            "epoch_1_train\n",
            "50 step processed.. current loss : 23.324275970458984\n",
            "epoch_1_train\n",
            "100 step processed.. current loss : 13.347858428955078\n",
            "Average Loss : 33.76944756715194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.57      0.56      0.57       609\n",
            "          LC       0.56      0.44      0.50       534\n",
            "          OG       0.45      0.18      0.25       963\n",
            "          PS       0.49      0.45      0.47       733\n",
            "          TI       0.29      0.22      0.25        94\n",
            "\n",
            "   micro avg       0.51      0.37      0.43      2933\n",
            "   macro avg       0.47      0.37      0.41      2933\n",
            "weighted avg       0.50      0.37      0.42      2933\n",
            "\n",
            "epoch_2_train\n",
            "50 step processed.. current loss : 12.08939266204834\n",
            "epoch_2_train\n",
            "100 step processed.. current loss : 7.872105598449707\n",
            "Average Loss : 11.48853117072064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.72      0.69      0.70       609\n",
            "          LC       0.69      0.51      0.59       534\n",
            "          OG       0.62      0.45      0.52       963\n",
            "          PS       0.69      0.52      0.59       733\n",
            "          TI       0.60      0.53      0.56        94\n",
            "\n",
            "   micro avg       0.67      0.53      0.59      2933\n",
            "   macro avg       0.67      0.54      0.59      2933\n",
            "weighted avg       0.67      0.53      0.59      2933\n",
            "\n",
            "epoch_3_train\n",
            "50 step processed.. current loss : 5.855278015136719\n",
            "epoch_3_train\n",
            "100 step processed.. current loss : 8.886910438537598\n",
            "Average Loss : 8.030036996758502\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.72      0.71      0.72       609\n",
            "          LC       0.67      0.59      0.63       534\n",
            "          OG       0.69      0.47      0.56       963\n",
            "          PS       0.57      0.64      0.60       733\n",
            "          TI       0.76      0.62      0.68        94\n",
            "\n",
            "   micro avg       0.66      0.59      0.62      2933\n",
            "   macro avg       0.68      0.61      0.64      2933\n",
            "weighted avg       0.67      0.59      0.62      2933\n",
            "\n",
            "epoch_4_train\n",
            "50 step processed.. current loss : 6.643741607666016\n",
            "epoch_4_train\n",
            "100 step processed.. current loss : 5.634779930114746\n",
            "Average Loss : 6.229154901919157\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.75      0.69      0.72       609\n",
            "          LC       0.71      0.60      0.65       534\n",
            "          OG       0.67      0.57      0.62       963\n",
            "          PS       0.66      0.62      0.64       733\n",
            "          TI       0.79      0.70      0.74        94\n",
            "\n",
            "   micro avg       0.69      0.62      0.65      2933\n",
            "   macro avg       0.71      0.64      0.67      2933\n",
            "weighted avg       0.69      0.62      0.65      2933\n",
            "\n",
            "epoch_5_train\n",
            "50 step processed.. current loss : 4.568780899047852\n",
            "epoch_5_train\n",
            "100 step processed.. current loss : 4.615495681762695\n",
            "Average Loss : 5.008639503561932\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.78      0.71      0.75       609\n",
            "          LC       0.68      0.63      0.66       534\n",
            "          OG       0.69      0.55      0.61       963\n",
            "          PS       0.67      0.63      0.65       733\n",
            "          TI       0.86      0.66      0.75        94\n",
            "\n",
            "   micro avg       0.71      0.62      0.66      2933\n",
            "   macro avg       0.74      0.64      0.68      2933\n",
            "weighted avg       0.71      0.62      0.66      2933\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENjD9mNs5B3U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sN0EmPiuPqO"
      },
      "source": [
        "## baseline\n",
        "- bi-GRU + CRF 모델 -> 2-layer\n",
        "\n",
        "\n",
        "## 시도해볼만한 것\n",
        "- deep layer\n",
        "- pos 인풋으로 넣기\n",
        "- CNN layer\n",
        "- pre train 임베딩\n",
        "- 후처리\n",
        "- 텐서보드 empirical 한 실험\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7sQkmudukQm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}