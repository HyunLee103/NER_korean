{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "토큰 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/train_token_1124.pickle', 'rb') as fs:\n",
    "    train_token = pickle.load(fs)"
   ]
  },
  {
   "source": [
    "khaiii를 이용한 형태소 분석 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi \n",
    "        \n",
    "def doPosTagging(sentence):\n",
    "    khai = KhaiiiApi()\n",
    "\n",
    "    morphs = []\n",
    "    for word in khai.analyze(sentence):\n",
    "        for morph in word.morphs:\n",
    "            morphs.append((morph.lex, morph.tag))\n",
    "            \n",
    "    return morphs\n",
    "\n",
    "train_senetence = list(map(' '.join, train_token))\n",
    "pos_tagged = list(map(doPosTagging, train_senetence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphPlusPos = lambda data: [pos for morph, pos in data]\n",
    "pos_feature = list(map(morphPlusPos, pos_tagged))"
   ]
  },
  {
   "source": [
    "품사 집합 만들기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = list(set([y for x in pos_feature for y in x]))\n",
    "pos_set.extend(['SF', 'SO', 'SWK'])\n",
    "pos_set = sorted(pos_set)"
   ]
  },
  {
   "source": [
    "One-hot encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posOnehot(sentence):\n",
    "    output = []\n",
    "    for word in sentence:\n",
    "        onehot = np.zeros(len(pos_set))\n",
    "        ind = pos_set.index(word)\n",
    "        onehot[ind] = 1 \n",
    "        output.append(onehot)\n",
    "    return output\n",
    "\n",
    "pos_onehot = list(map(posOnehot, pos_feature))"
   ]
  }
 ]
}